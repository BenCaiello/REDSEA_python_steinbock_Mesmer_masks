{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87093e13-0553-486c-84bb-4939bb836234",
   "metadata": {},
   "source": [
    "## REDSEA python version for Steinbock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1218a-1221-4e70-b7e2-50a9c2909c81",
   "metadata": {},
   "source": [
    "Edited from the original REDSEA python version 0.0.1 file in this folder (see that for details).\n",
    "\n",
    "Edited by Ben Caiello, with the intent of allowing it to work with current DeepCell segmentation masks / the steinbock pipeline. The algorithm is not identical, as that is not possible (I think) given the differenes in the masks of old DeepCell and new DeepCell (+ / - zero boundaries changes the effective pixel width of the boundary measurement step).\n",
    "\n",
    "Dataset I had been using for CD3 / CD20 compensation: https://zenodo.org/records/8023452\n",
    "\n",
    "Passed through steinbock to derive .tiffs and masks, then fed into this script\n",
    "\n",
    "The directory structure required for the script as-written is the one naturally produced by steinbock:\n",
    "\n",
    "A master directory with two folders: \\img & \\masks - each containing .tiffs with the original images and the DeepCell generated masks, repectively, with matching file names - and 1 .csv file (panel.csv). These are all naturally produced by steinbock when it is run.\n",
    "\n",
    "The script has not been thoroughly tested, but should produce outputs and seems to be doing what it is supposed to with the limited testing so far.\n",
    "\n",
    "\n",
    "Some (potential and certain) differences in the algorithm to note:\n",
      "\n",
    "    1. Since the masks format is different with DeepCell now (no cell-cell padding with 0's), the algorithm is made to find border px by looking for >1 segmentation label (indicating >1 cell or cell + background boundary)\n",
    "\n",    
    "     2. Also, because of the lack of padding, the effective distances and sizes of the diamond / square boder px identification step is altered"
    "\n",
    "Main thing I remain worried about: logic errors of zero-indexing from matlab --> python --> my python transition or other hard-to-spot errors"
     ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9238b26c-18cd-42e1-bbff-c7d0f17f019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import PIL\n",
    "from PIL import Image, ImageSequence, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage \n",
    "import skimage.measure\n",
    "import skimage.morphology\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import tifffile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "827bda19-7c0a-4d81-b25e-87cd83c76a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Edit these for your run!\n",
    "# file locations\n",
    "steinbockPath = 'C:\\\\Users\\\\caiello\\\\Desktop\\\\RedSEA practice\\\\practice' # main folder must contain \\img and \\masks folders and a panel.csv file (matching steinbock output)\n",
    "\n",
    "# or set file_name manually\n",
    "file_name = 'RNANeg_Tonsil_003'  #do not include the .tiff file type! If doing only one .tiff at a time, change this to your file name\n",
    "\n",
    "#  select which channel to normalize\n",
    "normChannels = [13,18]    # I use numbers here, can switch to names as in your panel file -- if you set \"channel_names_numbers = False\" in the the file_reader call in the next cell \n",
    "\n",
    "# parameters for compensation (change as desired)\n",
    "REDSEAChecker = 1 # 1 means subtract + reinforce\n",
    "elementShape = 1 # star, 1 == square size\n",
    "elementSize = 1 # star or square extension size\n",
    "\n",
    "\n",
    "# output path\n",
    "pathResults = steinbockPath + '/intensities' # output location, set as intensities here to match steinbock output\n",
    "try:\n",
    "    os.listdir(pathResults)\n",
    "except:\n",
    "    os.mkdir(pathResults)\n",
    "\n",
    "alt_out_path = \"\\\\an\\\\alternate\\folder\\\\in\\\\which\\\\to\\\\put\\\\your\\\\non-scaled\\\\and\\\\non-compensated\\\\data\" ## use if you want the non-compensated / non-scaled data, but don't want them crowding the main steinbock intensities folder\n",
    "######### if you use alt_path, manually make the directory in file explorer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d5bb310-9f55-4551-a9d0-89f65c7fbf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |███████████████████████████████████████████████████████████████████████████████████████████████████-| 100.0% Complete\r"
     ]
    }
   ],
   "source": [
    "## single file run\n",
    "clusterChannels, Full_Tiff, Segmentation = file_reader(steinbockPath, file_name)\n",
    "cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum = cell_statistics(clusterChannels, Full_Tiff, Segmentation)\n",
    "cellPairNorm = cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1)\n",
    "MIBIdataNearEdge1, MIBIdataNearEdgeAvg = Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = 2, elementSize = 2)\n",
    "dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells = CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannels, cellNum, cellSizes, Border_average = False, MIBIdataNearEdgeAvg = MIBIdataNearEdgeAvg)\n",
    "\n",
    "#Outputter(dataScaleSizeCells, file_name = file_name + \"_pre_REDSEA_scaled\", pathResults = pathResults)\n",
    "Outputter(dataCompenScaleSizeCells, file_name, pathResults = pathResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce749e62-0b97-4baf-aab5-3ef69d636233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RedSEA compensation of file: RNANeg_Tonsil_001\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_002███████████████████████████████████████████████████████-| 100.0% Complete\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_003███████████████████████████████████████████████████████-| 100.0% Complete\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_004███████████████████████████████████████████████████████-| 100.0% Complete\n",
      "Progress: |███████████████████████████████████████████████████████████████████████████████████████████████████-| 100.0% Complete\r"
     ]
    }
   ],
   "source": [
    "# for multiple images at once:\n",
    "file_list = []\n",
    "for i in os.listdir(steinbockPath + \"\\\\img\"):\n",
    "    i = i.replace(\".tiff\",\"\")\n",
    "    file_list.append(i) \n",
    "\n",
    "for i in file_list:           # iterate over files in directory\n",
    "    file_name = i\n",
    "    print(\"Starting RedSEA compensation of file: \" + file_name)\n",
    "    clusterChannels, Full_Tiff, Segmentation = file_reader(steinbockPath, file_name)\n",
    "    cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum = cell_statistics(clusterChannels, Full_Tiff, Segmentation)\n",
    "    cellPairNorm = cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1)\n",
    "    MIBIdataNearEdge1, MIBIdataNearEdgeAvg = Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = 2, elementSize = 2)\n",
    "    dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells = CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannels, cellNum, cellSizes, Border_average = False, MIBIdataNearEdgeAvg = 0)\n",
    "    ################ Change / add Outputter calls if you want to write dataframes besides the scaled cells and compensated scaled cells:  \n",
    "    Outputter(dataScaleSizeCells, file_name = file_name + \"_pre_REDSEA_scaled\", pathResults = pathResults)\n",
    "    Outputter(dataCompenScaleSizeCells, file_name, pathResults = pathResults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92f14f73-9593-40d4-8621-73e87b4b6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions (normally, minimize this cell)\n",
    "# helper function 1\n",
    "def ismember(a, b):\n",
    "    bind = {}\n",
    "    for i, elt in enumerate(b):\n",
    "        if elt not in bind:\n",
    "            bind[elt] = i\n",
    "    return [bind.get(itm, None) for itm in a]  # None can be replaced by any other \"not in b\" value\n",
    "\n",
    "# helper function 2\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    '''This prints a progress bar'''\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "## From now these are BPC edited functions\n",
    "def file_reader(steinbockPath, file_name, ends_with_tiff = False, channel_names_numbers = True):\n",
    "    '''\n",
    "    This function reads in the panel file, the image, and its matching segmentation image from your Steinbock output folder.\n",
    "\n",
    "    args:\n",
    "    steinbockPath  -- this is the directory path to your steinbock output\n",
    "    file_name -- this is the name of the ROI / image to be read.\n",
    "    ends_with_tiff (default = False) -- this determines whether you include the .tiff file type in the file_name. By default, do not include .tiff\n",
    "    channel_names_numbers (default = True) -- this determines whether it returns the Channels from the panel file as numbered indices (default) or as names\n",
    "\n",
    "    returns:\n",
    "    clusterChannels -- the channels from the panel file kept by the steinbock program\n",
    "    Full_Tiff -- a multi-dimensional numpy array representing all of the channels of the .tiff image\n",
    "    Segmentation -- a 2D numpy array representing the segmentation by DeepCell\n",
    "    '''\n",
    "    ## panel file\n",
    "    if ends_with_tiff == True:\n",
    "        file_name = file_name\n",
    "    elif ends_with_tiff == False:\n",
    "        file_name = file_name + \".tiff\"\n",
    "    \n",
    "    massDS_path = steinbockPath + '\\\\panel.csv' # csv file location, need\n",
    "    massDS = pd.read_csv(massDS_path) # read the mass csv\n",
    "    if channel_names_numbers == True:\n",
    "        clusterChannels = massDS[massDS['keep'] == 1].reset_index().index.values\n",
    "    else:\n",
    "        clusterChannels = massDS['name']  # if you want your channel names to be what is used\n",
    "\n",
    "    # Tiff file\n",
    "    pathTiff = steinbockPath + '\\\\img\\\\' + file_name #  tiff location, links to a single .tiff\n",
    "    array_list=[]\n",
    "    for channel in clusterChannels:\n",
    "        t=tifffile.imread(pathTiff)[channel]\n",
    "        array_list.append(t)\n",
    "    Full_Tiff=np.stack(array_list,axis=2) # count matrices in the image\n",
    "\n",
    "    ## Segmentation file\n",
    "    pathMat = steinbockPath + '\\\\masks\\\\' + file_name   # corresponding .tiff's mask\n",
    "    Segmentation = tifffile.imread(pathMat).astype('int')\n",
    "\n",
    "    return clusterChannels, Full_Tiff, Segmentation\n",
    "\n",
    "def cell_statistics(clusterChannels, Full_Tiff, Segmentation):\n",
    "    ''' This recovers key statistics from the files: cell / channel number, cell sizes & data, and image dimensions'''\n",
    "    channelNum = len(clusterChannels) # how many channels\n",
    "\n",
    "    cellNum = np.max(Segmentation) # how many labels\n",
    "    stats = skimage.measure.regionprops(Segmentation) # get the regional props for all the labels\n",
    "    \n",
    "    ### make empty container matrices\n",
    "    data = np.zeros((cellNum,channelNum))\n",
    "    dataScaleSize = np.zeros((cellNum,channelNum))\n",
    "    cellSizes = np.zeros((cellNum,1))\n",
    "\n",
    "    # this part extract counts data from the whole cell regions, for each individual cells etc\n",
    "    \n",
    "    for i in range(cellNum): # for each cell (label)\n",
    "        label_counts=[Full_Tiff[coord[0],coord[1],:] for coord in stats[i].coords] # all channel count for this cell\n",
    "        data[i,0:channelNum] = np.sum(label_counts, axis=0) #  sum the counts for this cell\n",
    "        dataScaleSize[i,0:channelNum] = np.sum(label_counts, axis=0) / stats[i].area # scaled by size\n",
    "        cellSizes[i] = stats[i].area # cell sizes\n",
    "    \n",
    "    [rowNum, colNum] = Segmentation.shape      #could also have been Full_Tiff.shape\n",
    "    \n",
    "    return cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum\n",
    "\n",
    "\n",
    "def cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1):\n",
    "    ''' Identifies cell-cell contacts, creating a matrix representing the number and identity of each cell's contact with other cells.\n",
    "\n",
    "    args:\n",
    "    Segmentation -- this is the numpy array representation of the DeepCell segmentation\n",
    "    cellNum -- this is the number of cells in the segmentation\n",
    "    rowNum -- the number of rows of pixels in the image\n",
    "    colNum -- the number of columns of pixels in the image\n",
    "    REDSEAChecker -- this determines whether the the algorithm reinforces a cell's number back to itself & compensates (1) or only removes signal with the compensation (0). Defaults to 1. \n",
    "\n",
    "    returns:\n",
    "    cellPairNorm -- this is the cell-cell contact matrix used for compensation\n",
    "    '''\n",
    "    cellPairMap = np.zeros(((cellNum + 1),(cellNum + 1))) # cell-cell shared perimeter matrix container\n",
    "    \n",
    "    # start looping the mask and produce the cell-cell contact matrix\n",
    "    for i in range(rowNum):\n",
    "        if i == 0:   # these conditional statements account for pixels on the edge of the image by shrinking the 3x3 box to smaller dimensions\n",
    "            a = 0\n",
    "            c = 2\n",
    "        elif i == (rowNum - 1):\n",
    "            a = 1\n",
    "            c = 1\n",
    "        else:\n",
    "            a = 1\n",
    "            c = 2\n",
    "        for j in range(colNum):\n",
    "            if j == 0:\n",
    "                b = 0\n",
    "                d = 2\n",
    "            elif j == (colNum - 1):\n",
    "                b = 1\n",
    "                d = 1\n",
    "            else:\n",
    "                b = 1\n",
    "                d = 2\n",
    "            tempMatrix = Segmentation[i-a:i+c,j-b:j+d] # the 3x3 window, centered on the point i,j\n",
    "            #print(tempMatrix)\n",
    "            tempFactors = np.unique(tempMatrix).astype('int') #unique\n",
    "            #print(tempFactors)\n",
    "            centerpoint_value = Segmentation[i,j]\n",
    "            #print(centerpoint_value)\n",
    "            for k in tempFactors:\n",
    "                if k != centerpoint_value: # only add to the cellPairMap for the centerpoint pixel -- this prevents multiplicate counting\n",
    "                    #print(\"trigger\")\n",
    "                    cellPairMap[centerpoint_value,k] = cellPairMap[centerpoint_value,k] + 1  \n",
    "        \n",
    "    # converting the cell cell maps to fraction of cell - cell boundary (not of total cell boundary [!?] -- as in boundary with empty space not counted)\n",
    "    cellPairNorm = np.zeros(((cellNum+1),(cellNum+1)))\n",
    "    for i in np.arange(0,len(cellPairMap)):\n",
    "        if np.sum(cellPairMap[i]) > 0:\n",
    "            cellPairNorm[i] = - (cellPairMap[i] / np.sum(cellPairMap[i]))\n",
    "        #else:\n",
    "            #print(\"this shouldn't happen\")\n",
    "    cellPairNorm = cellPairNorm[1:,1:]\n",
    "    cellPairNorm = cellPairNorm + REDSEAChecker*np.identity(cellNum ) \n",
    "    return cellPairNorm\n",
    "\n",
    "def Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = 2, elementSize = 2):\n",
    "    '''\n",
    "    This generates the intensities of each cell's border pixels in every channel for use in the compensation\n",
    "    '''\n",
    "    MIBIdataNearEdge1 = np.zeros((cellNum+1,channelNum))\n",
    "    MIBIdataNearEdgeAvg = np.zeros((cellNum+1,channelNum))\n",
    "    #initialize progress bar:\n",
    "    items = list(range(cellNum + 1))\n",
    "    l = len(items)\n",
    "    printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 100) # progress bar\n",
    "    \n",
    "    # Square or Diamond shape of desired size\n",
    "    if elementShape==1: # square\n",
    "        shape=skimage.morphology.square((2*elementSize) + 1)\n",
    "    elif elementShape==2: # diamond\n",
    "        shape=skimage.morphology.diamond(elementSize) # create diamond shapte based on elementSize\n",
    "    else:\n",
    "        print(\"Error elementShape Value not recognized.\")   \n",
    "\n",
    "    # iterate over cells, measuring total border intensity\n",
    "    for i in range(cellNum + 1) :\n",
    "        if i == 0:\n",
    "            continue\n",
    "        num_border_px = 0\n",
    "        [tempRow,tempCol] = np.asarray(Segmentation==i).nonzero()\n",
    "        [rowNum, colNum] = Segmentation.shape\n",
    "        # sequence in row not col, should not affect the code\n",
    "        for j in range(len(tempRow)):\n",
    "            label_in_shape=[] # empty list in case\n",
    "            if (tempRow[j] - elementSize) <= -1:   # these conditionals are for ensuring the tempMatrix does not cross the image edge\n",
    "                a = 0 + tempRow[j]\n",
    "                c = 1 + elementSize\n",
    "            elif (tempRow[j] + elementSize) > rowNum - 1:\n",
    "                a = 0 + elementSize\n",
    "                c = 1 + rowNum - tempCol[j]\n",
    "            else:\n",
    "                a = 0 + elementSize\n",
    "                c = 1 + elementSize\n",
    "            if (tempCol[j] - elementSize) <= -1:\n",
    "                b = 0 + tempCol[j]\n",
    "                d = 1 + elementSize\n",
    "            elif (tempCol[j] + elementSize) > colNum - 1:\n",
    "                b = 0 + elementSize\n",
    "                d = 1 + colNum - tempCol[j]\n",
    "            else:\n",
    "                b = 0 + elementSize\n",
    "                d = 1 + elementSize\n",
    "            tempMatrix = Segmentation[tempRow[j]-a:tempRow[j]+c,tempCol[j]-b:tempCol[j]+d] # the 3x3 window, centered on the point i,j\n",
    "            if elementShape == 2:\n",
    "                [dim1, dim2] = tempMatrix.shape\n",
    "                reducedShape = shape[:dim1,:dim2].astype('bool')\n",
    "                reducedMatrix = tempMatrix[reducedShape]   # the reduced shape /  matrix lines are there to accommodate the diamond shape\n",
    "                tempFactors = np.unique(reducedMatrix).astype('int')\n",
    "            else:  # no need for reduction if using the square shape\n",
    "                tempFactors = np.unique(tempMatrix).astype('int')\n",
    "            if len(tempFactors) > 1:\n",
    "                num_border_px += 1\n",
    "                MIBIdataNearEdge1[i,:] = MIBIdataNearEdge1[i,:] + Full_Tiff[tempRow[j],tempCol[j],:]\n",
    "        MIBIdataNearEdgeAvg[i,:] =  MIBIdataNearEdge1[i,:] / num_border_px\n",
    "        # Update Progress Bar\n",
    "        if ((i % 500) == 0) or (i == (cellNum - 1)):\n",
    "            printProgressBar(i + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 100)\n",
    "    MIBIdataNearEdge1 = MIBIdataNearEdge1[1:,:]\n",
    "    MIBIdataNearEdgeAvg = MIBIdataNearEdgeAvg[1:,:]\n",
    "    return MIBIdataNearEdge1, MIBIdataNearEdgeAvg\n",
    "            \n",
    "def CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannels, cellNum, cellSizes, Border_average = False, MIBIdataNearEdgeAvg = 0):  \n",
    "    '''\n",
    "    Takes the cell-cell matrix and border intensity matrix and calculates compensation.\n",
    "\n",
    "    args:\n",
    "    MIBIdataNearEdge1 -- cell border intensity matrix\n",
    "    cellPairNorm -- cell-cell contact matrix\n",
    "    data -- raw intensity measured for each cell in each channel\n",
    "    dataScaleSize -- the raw intensities of each normalized to cell size\n",
    "    normChannels -- the channels you wish to compensate with REDSEA\n",
    "    clusterChannels -- the channels in the dataset\n",
    "    cellNum -- the number of cell in the image\n",
    "    cellSizes -- the sizes of each cell\n",
    "    Border_average -- whether to use the raw border intensity of each cell to compensate the raw intensity of that cell (default), or the size normalized border intensity and size normalized whole cell intensity\n",
    "    MIBIdataNearEdgeAvg -- cell border intensity matrix, but with border intensities normalized by number of pixels (size of border regions). Only use with Border_average = True\n",
    "    \n",
    "    returns:\n",
    "    Four pandas dataframes, ready for export to csv's or examination in jupyter lab:\n",
    "    dataCells -- uncompensated (pre-REDSEA) cell intensity values\n",
    "    dataScaleSizeCells -- uncompensated cell intensities, normalized by cell size\n",
    "    dataCompenCells -- compensated cell intensity values\n",
    "    dataCompenScaleSizeCells -- compensated cell intensity values, normalized by cell sizes\n",
    "    '''\n",
    "    normChannelsInds = ismember(normChannels,clusterChannels)\n",
    "    channelNormIdentity = np.zeros((len(clusterChannels),1))\n",
    "    # make a flag for compensation\n",
    "    for i in range(len(normChannelsInds)):\n",
    "            channelNormIdentity[normChannelsInds[i]] = 1 \n",
    "    if Border_average == False:\n",
    "        MIBIdataNorm2 = np.transpose(np.dot(np.transpose(MIBIdataNearEdge1[:,:]),cellPairNorm))\n",
    "        #this is boundary signal subtracted by cell neighbor boundary\n",
    "        MIBIdataNorm2 = MIBIdataNorm2 + data # reinforce onto the whole cell signal (original signal)\n",
    "        \n",
    "        MIBIdataNorm2[MIBIdataNorm2<0] = 0 # clear out the negative ones\n",
    "        # flip the channelNormIdentity for calculation\n",
    "        rev_channelNormIdentity=np.ones_like(channelNormIdentity)-channelNormIdentity\n",
    "        # composite the normalized channels with non-normalized channels\n",
    "        # MIBIdataNorm2 is the matrix to return\n",
    "        MIBIdataNorm2 = data * np.transpose(np.tile(rev_channelNormIdentity,(1,cellNum))) + MIBIdataNorm2 * np.transpose(np.tile(channelNormIdentity,(1,cellNum)))\n",
    "        \n",
    "        # the function should return 4 variables\n",
    "        dataCells = data\n",
    "        dataScaleSizeCells = dataScaleSize\n",
    "        dataCompenCells = MIBIdataNorm2\n",
    "        dataCompenScaleSizeCells = MIBIdataNorm2 / cellSizes\n",
    "    elif Border_average == True:\n",
    "        MIBIdataNorm2 = np.transpose(np.dot(np.transpose(MIBIdataNearEdgeAvg),cellPairNorm))\n",
    "        MIBIdataNorm2 = MIBIdataNorm2 + dataScaleSize\n",
    "        MIBIdataNorm2[MIBIdataNorm2<0] = 0\n",
    "        rev_channelNormIdentity=np.ones_like(channelNormIdentity)-channelNormIdentity\n",
    "        MIBIdataNorm2 = data * np.transpose(np.tile(rev_channelNormIdentity,(1,cellNum))) + MIBIdataNorm2 * np.transpose(np.tile(channelNormIdentity,(1,cellNum)))\n",
    "        dataCells = dataScaleSize * cellSizes\n",
    "        dataScaleSizeCells = dataScaleSize\n",
    "        dataCompenCells = MIBIdataNorm2 * cellSizes\n",
    "        dataCompenScaleSizeCells = MIBIdataNorm2\n",
    "    list_of_arrays = [dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells]\n",
    "    for jj,j in enumerate(list_of_arrays):\n",
    "        labelVec = [i for i in np.arange(1,cellNum + 1,1)]\n",
    "        cellSizesVec_flat = [item for sublist in cellSizes for item in sublist] # flat the list\n",
    "        dataL = pd.DataFrame({'Object':labelVec})\n",
    "        dataCells_df=pd.DataFrame(j)\n",
    "        dataCells_df.columns = clusterChannels\n",
    "        if jj == 0:\n",
    "            dataCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 1:\n",
    "            dataScaleSizeCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 2:\n",
    "            dataCompenCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 3:\n",
    "            dataCompenScaleSizeCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "    return dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells\n",
    "\n",
    "def Outputter(data_in, file_name, pathResults = pathResults):\n",
    "    data_in.to_csv(pathResults + '\\\\' + file_name + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c440d-1b1d-4b24-a279-97bcf883532c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02f6fe-2513-401b-ab9d-ab3cfe5e91ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
